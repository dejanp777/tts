â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸš€ OpenRouter LLM Benchmark - Roleplay/Conversational Models
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   Testing 10 models with 3 runs each
   System prompt: "You are a 24-year-old secretary working in New Yor..."
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{
  "timestamp": "2025-12-15T22:48:48.888Z",
  "runsPerModel": 3,
  "modelsCount": 10,
  "results": [
    {
      "rank": 1,
      "model": "mistralai/mistral-7b-instruct",
      "name": "Mistral 7B Instruct",
      "category": "fast",
      "ttftAvg": 284,
      "ttftMin": 241,
      "ttftMax": 328,
      "totalAvg": 1042,
      "successRate": "3/3",
      "sampleResponses": [
        "",
        "Why did the scarecrow win a prize?\n<s> Because he was outstanding in his field! [/s]"
      ]
    },
    {
      "rank": 2,
      "model": "meta-llama/llama-3.1-8b-instruct",
      "name": "Llama 3.1 8B Instruct",
      "category": "fast",
      "ttftAvg": 430,
      "ttftMin": 132,
      "ttftMax": 1010,
      "totalAvg": 558,
      "successRate": "3/3",
      "sampleResponses": [
        "I'm fabulous, darling. Your day's looking up.",
        "Why was math book sad?"
      ]
    },
    {
      "rank": 3,
      "model": "qwen/qwen-2.5-72b-instruct",
      "name": "Qwen 2.5 72B Instruct",
      "category": "conversational",
      "ttftAvg": 453,
      "ttftMin": 355,
      "ttftMax": 550,
      "totalAvg": 629,
      "successRate": "3/3",
      "sampleResponses": [
        "Fabulous, as always. What about you?",
        "Why don't scientists trust atoms?"
      ]
    },
    {
      "rank": 4,
      "model": "google/gemini-2.0-flash-001",
      "name": "Gemini 2.0 Flash",
      "category": "fast",
      "ttftAvg": 460,
      "ttftMin": 319,
      "ttftMax": 555,
      "totalAvg": 483,
      "successRate": "3/3",
      "sampleResponses": [
        "Dying for a raise, you? ğŸ˜‰",
        "Why don't scientists trust atoms?"
      ]
    },
    {
      "rank": 5,
      "model": "gryphe/mythomax-l2-13b",
      "name": "MythoMax L2 13B",
      "category": "roleplay",
      "ttftAvg": 1321,
      "ttftMin": 1012,
      "ttftMax": 1839,
      "totalAvg": 2220,
      "successRate": "3/3",
      "sampleResponses": [
        "Fabulous, thanks for asking!",
        "Why did the tomato turn red? Because it saw the salad dressing!"
      ]
    },
    {
      "rank": 6,
      "model": "deepseek/deepseek-chat-v3-0324",
      "name": "DeepSeek Chat V3 (BASELINE)",
      "category": "baseline",
      "ttftAvg": 1575,
      "ttftMin": 591,
      "ttftMax": 3102,
      "totalAvg": 3789,
      "successRate": "3/3",
      "sampleResponses": [
        "Fabulous as always, darling. You?",
        "Why donâ€™t eggs tell jokes? Theyâ€™d crack up!"
      ]
    },
    {
      "rank": 7,
      "model": "nousresearch/hermes-3-llama-3.1-70b",
      "name": "Hermes 3 Llama 70B",
      "category": "conversational",
      "ttftAvg": 1584,
      "ttftMin": 1160,
      "ttftMax": 2396,
      "totalAvg": 2708,
      "successRate": "3/3",
      "sampleResponses": [
        "Living the dream, darling. What's the buzz?",
        "Why don't scientists trust atoms? Because they make up everything!"
      ]
    }
  ],
  "recommendation": {
    "model": "mistralai/mistral-7b-instruct",
    "name": "Mistral 7B Instruct",
    "ttftAvg": 284,
    "improvementVsBaseline": "82%",
    "envConfig": "OPENROUTER_MODEL=mistralai/mistral-7b-instruct"
  },
  "failed": [
    {
      "model": "anthropic/claude-3-haiku-20240307",
      "name": "Claude 3 Haiku"
    },
    {
      "model": "neversleep/llama-3-lumimaid-70b",
      "name": "Llama 3 Lumimaid 70B"
    },
    {
      "model": "cognitivecomputations/dolphin-mixtral-8x22b",
      "name": "Dolphin Mixtral 8x22B"
    }
  ]
}
